{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Imports and Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(data, title=None):\n",
    "    \"\"\"Uses seaborn to plot a correlation matrix of the input data\"\"\"\n",
    "    \n",
    "    corr_unsorted = data.corr().values\n",
    "    order = np.array(hierarchy.dendrogram(hierarchy.ward(corr_unsorted), no_plot=True)['ivl'], dtype=\"int\")\n",
    "    corr = corr_unsorted[order, :][:, order]\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True,\n",
    "               xticklabels=data.columns[order],\n",
    "               yticklabels=data.columns[order])\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coef(estimator, features, estimator_name=None, title=None, num_features=None):\n",
    "    \"\"\"Function to plot the coefficients of a fitted linear estimator\"\"\"\n",
    "    \n",
    "    # Grab the estimator from a GridSearchCV object if necessary\n",
    "    if estimator_name:\n",
    "        pipe = estimator.best_estimator_\n",
    "        estimator = pipe.named_steps[estimator_name]\n",
    "        \n",
    "    features_short = [x[8:] for x in features]\n",
    "    \n",
    "    # Transform for poly feature names if applicable\n",
    "    try:\n",
    "        features_final = pipe.named_steps['poly'].get_feature_names(features_short)\n",
    "    \n",
    "    except KeyError:\n",
    "        features_final = features_short\n",
    "    \n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': features_final,\n",
    "        'coefficient': estimator.coef_,\n",
    "        'abs': abs(estimator.coef_)\n",
    "                           }) \\\n",
    "        .sort_values('abs', ascending=False)\n",
    "    \n",
    "    if num_features:\n",
    "        coef_df = coef_df.iloc[:num_features]\n",
    "    \n",
    "#     plt.figure(figsize=(20,10))\n",
    "#     sns.set()\n",
    "    return sns.barplot(x='coefficient', y='feature', data=coef_df, orient='h')\n",
    "#     plt.show()\n",
    "#     plt.title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_plot(perm, features, title=None):\n",
    "    \"\"\"Creates a bar plot of permutation importances from a fitted eli5.sklearn.PermutationImportance object\"\"\"\n",
    "    \n",
    "    perm_results = np.mean(np.array(perm.results_), axis=0)\n",
    "    perm_df = pd.DataFrame({\n",
    "        'feature': [x[8:] for x in features],\n",
    "        'importance': perm_results\n",
    "                       })\n",
    "    \n",
    "#     plt.figure(figsize=(20,10))\n",
    "#     sns.set()\n",
    "    return sns.catplot(y='feature', x='importance', data=perm_df, kind='bar', orient='h');\n",
    "#     plt.show()\n",
    "#     plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reading and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(r'../data/processed/X_priority.csv').iloc[:, 1:]\n",
    "y = pd.read_csv(r'../data/processed/y_priority.csv').iloc[:, 1]\n",
    "\n",
    "# Drop null values\n",
    "X.dropna(how='all', inplace=True)\n",
    "y = y.reindex_like(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Check that X and y have the same values after dropping NA values: {}'.format(len(X) == len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**m10_cen_memi_x** is the only variable that has missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target actually looks pretty close to normally distributed which will likely be helpful for linear methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_features = \"m10_cen_memi_x\"\n",
    "cont_features = [x for x in X.columns.tolist() if x !=bin_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[cont_features].hist(figsize=(20,20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of these distributions have very strong left skew, and look vaguely exponential. A few of them look like they may be normal. NETS variables are the most strikingly right skewed, with the vast majority of tracts having a very small number of each kind of business.  \n",
    "\n",
    "The LTDB variables are not quit as striking, and seem to have a lot more tract-level variety. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Feature(s) \\[Needs filling in\\]\n",
    "\n",
    "The only binary feature is m10_cen_memi_x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-wise Plots vs Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows=16\n",
    "ncols=3\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20,55));\n",
    "k = 0\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        try:\n",
    "            feature = X[cont_features].iloc[:, k]\n",
    "            axes[i, j].set(xlabel=feature.name,\n",
    "                          ylabel=\"Life Expectancy\");\n",
    "            axes[i, j].scatter(x=feature, y=y, alpha=0.3);\n",
    "            k += 1\n",
    "        except IndexError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these plots, **especially** the NETS ones, show heteroskedacicity.  We're not very concerened with p-values and confidence intervals so it may not be a problem but it is definitely something to note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collinearities\n",
    "\n",
    "We're expecting a lot of collinearity, especially within LTDB and census varuables. I attempted a pairplot, but there were too many variables for this to be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "\n",
    "from eli5 import explain_weights, show_weights\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "from scipy.cluster import hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[cont_features], y, random_state=0)\n",
    "\n",
    "# Define a repeatable Cross-Validation Generator\n",
    "cv_5 = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for col in cont_features:\n",
    "    lr = LinearRegression()\n",
    "    score = cross_val_score(lr, X_train[col].values.reshape(-1,1), y_train, cv=cv_5, scoring='r2')\n",
    "    results[col] = np.mean(score)\n",
    "\n",
    "results_df = pd.Series(results).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some pretty good univariate predictors.  As expected, all of the top performers are LTDB variables.  The following LTDB variables all produce an R<sup>2</sup> of > 0.3:\n",
    "* **hinci_m**: Median household income\n",
    "* **col_p**: Percent of persons with at least a 4-year college degree\n",
    "* **npov_p**: Percent of persons in poverty\n",
    "* **hs_p**: Percent of persons with a high school degree or less\n",
    "* **mhmvali_m**: Medidan home value (inflation-adjusted)\n",
    "\n",
    "I suspect that many of these variables will be highly correlated, so I'll look at a covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_r_cols = results_df[results_df > .3].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[high_r_cols].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr(X_train[high_r_cols], title='High R^2 Value Feature Correlation Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do see some pretty high correlations, but not as high as I was expecting in most cases. The highest is a negative correlation between the  number of percent of people with at least a college degree and the percent of people with at most a high school degree, which are obviously in direct opposition.  There's also a pretty strong positive correlation between college degrees and median household income, which once again isn't surprising and I imagine most of these things are pretty well studied already.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1\n",
    "\n",
    "Features will only include the intersection of Census and ACS variables used in USALEEPs model to impute life expectancy for age-bands with death counts of 0 and those which are found in the RECVD priority dataset. These include median household income, population density, and the proportions of the population that are non-Hispanic black, Hispanic, and had a 4-year college degree or higher in the census tract.\n",
    "\n",
    "**Full variable names**:\n",
    "- t10_ldb_hinc_m\n",
    "- t10_ldb_pop_d\n",
    "- t10_ldb_nhblk_p\n",
    "- t10_ldb_hisp_p\n",
    "- t10_ldb_col_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models\n",
    "\n",
    "We'll create a phase 1 model using Ridge regression, LASSO regression, and ElasticNet. The baseline model will include all variables except NETS variables. We'll try both StandardScaler and RobustScaler (which uses the median and quartiles to reduce the influence of outliers on scaling). Due to the high correlation between variables, we won't use standard Ordinary Least Squares Regression as we won't be able to trust the coefficients, which is our initial method of identifying variable importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(estimator, param_grid, cv, X_train, y_train, phase):\n",
    "    \"\"\"Functional code to perform all operations for a Linear Model and return a fitted GridSearchCV\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    estimator -- The scikit-learn estimator to use\n",
    "    param_grid -- Dict object detailing parameters for the model\n",
    "    cv -- the cross-validation generator object\n",
    "    X_train -- training data features\n",
    "    y_train -- training data outcome\n",
    "    phase -- Which phase of analysis this is for labeling charts, as a string (Ex: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    est_str = str(estimator)\n",
    "    est_name = est_str[:est_str.index('(')]\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('poly', PolynomialFeatures()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('estimator', estimator)\n",
    "    ])\n",
    "    \n",
    "    grid = GridSearchCV(pipe, param_grid, cv=cv, scoring='r2')\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    train_results = pd.DataFrame(grid.cv_results_)\n",
    "    \n",
    "    #get the best linear estimator for comparison\n",
    "    lin_best = train_results[train_results['param_poly__degree'] == 1] \\\n",
    "    .sort_values('rank_test_score') \\\n",
    "    .iloc[0] \\\n",
    "    .mean_test_score\n",
    "    \n",
    "    # Print results\n",
    "    print(\n",
    "    \"Phase 1 {} R^2 Value: {}\\n\\\n",
    "Best Parameters: {}\\n\\n\\\n",
    "Best R^2 Value with No Polynomial Features or Interactions: {}\\n\" \\\n",
    "    .format(est_name, grid.best_score_, grid.best_params_, lin_best)\n",
    ")\n",
    "    \n",
    "#     Coefficient graphs\n",
    "    plt.figure(figsize=(20,10));\n",
    "    sns.set()\n",
    "    coef_plot = plot_coef(grid,\n",
    "              X_train.columns,\n",
    "              estimator_name='estimator',\n",
    "              num_features=20)\n",
    "    plt.title('Phase 1 {} Coefficients'.format(est_name))\n",
    "    plt.show()\n",
    "   \n",
    "    \n",
    "    #permutation importance\n",
    "    perm = PermutationImportance(grid.best_estimator_, scoring='r2') \\\n",
    "        .fit(X_train, y_train, cv='prefit')\n",
    "    \n",
    "    plt.figure(figsize=(20,10));\n",
    "    plot_perm = perm_plot(perm, X_train.columns.tolist())\n",
    "    plt.title('Phase {} {} Permutation Importance'.format(phase, est_name))\n",
    "    plt.show()\n",
    "    \n",
    "    display(show_weights(perm, feature_names=X_train.columns.tolist()))    \n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_features = ['t10_ldb_hinci_m',\n",
    "    't10_ldb_pop_d',\n",
    "    't10_ldb_nhblk_p',\n",
    "    't10_ldb_hisp_p',\n",
    "    't10_ldb_col_p']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation matrix for Phase 1 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr(X_train[p1_features], title='Phase 1 Features Correlation Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a 0.75 correlation between median household income and percent with a four year degree, which is pretty obvious and to be expected.  Other than this the correlations between variables aren't as drastic as I had imagined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO Regression\n",
    "\n",
    "Lasso regression performs L1 regularization, which adds a penalty equal to the absolute value of the magnitude of coefficients. This type of regularization can result in sparse models with few coefficients; Some coefficients can become zero and eliminated from the model. Larger penalties result in coefficient values closer to zero, which is the ideal for producing simpler models.\n",
    "\n",
    "![alt text](https://qph.fs.quoracdn.net/main-qimg-2a88e2acc009fa4de3edeb51e683ca02.webp)\n",
    "\n",
    "As shown in the image, the nature of the l1 norm function and its use of the absolute value drives coefficients directly to 0, while l2 norm used for ridge regression just drives them *close* to 0.  \n",
    "\n",
    "I'll be gridsearching for the optimum $\\alpha$ parameter on a log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_grid = {\n",
    "    'estimator__alpha': np.logspace(-3, 3, 5),\n",
    "    'poly__degree': [1, 2, 3]\n",
    "             }\n",
    "p1_lasso = linear_model(Lasso(), lasso_grid, cv_5, X_train[p1_features], y_train, '1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best model used the smallest regularization parameter.  we also see a .065 increase in R^2 when we use a model with 3rd order polynomials and interaction terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous conclusion is emphasized by viewing the coefficients; LASSO has set none of them to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutation importances largely agree with the LASSO Coefficients as effect estimates, assigning high importances to median household income, percentage of those with a 4-year degree or higher, and proportion of non-hispanic black residents. This also has an interpretability advantage that it assigns importance based on the original features despite us using polynomial features and interactions.  This is because in computing the permutation importance, each feature is randomly permuted before polynomial features and itneractions are taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from [eli5](https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html)  \n",
    "\n",
    "**Algorithm:**  \n",
    "The idea is the following: feature importance can be measured by looking at how much the score (accuracy, F1, R^2, etc. - any score we’re interested in) decreases when a feature is not available.\n",
    "\n",
    "To do that one can remove feature from the dataset, re-train the estimator and check the score. But it requires re-training an estimator for each feature, which can be computationally intensive. Also, it shows what may be important within a dataset, not what is important within a concrete trained model.\n",
    "\n",
    "To avoid re-training the estimator we can remove a feature only from the test part of the dataset, and compute score without using this feature. It doesn’t work as-is, because estimators expect feature to be present. So instead of removing a feature we can replace it with random noise - feature column is still there, but it no longer contains useful information. This method works if noise is drawn from the same distribution as original feature values (as otherwise estimator may fail). The simplest way to get such noise is to shuffle values for a feature, i.e. use other examples’ feature values - this is how permutation importance is computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge\n",
    "\n",
    "Ridge regression performs L2 regularization, which adds a penalty equal to the square of the magnitude of coefficients. This type of regularization will push values very close to 0, but will none will actually reach 0. Larger penalties result in coefficient values closer to zero, which is the ideal for producing simpler models.\n",
    "\n",
    "Refer to the image shown in the LASSO section: the nature of the l1 norm function and its use of the absolute value drives coefficients directly to 0, while l2 norm used for ridge regression just drives them *close* to 0.  \n",
    "\n",
    "I'll be using the gridsearching for the optimum $\\alpha$ parameter on a log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_grid = {\n",
    "    'estimator__alpha': np.logspace(-3, 3, 5),\n",
    "    'poly__degree': [1, 2, 3]\n",
    "             }\n",
    "p1_ridge = linear_model(Ridge(), ridge_grid, cv_5, X_train[p1_features], y_train, '1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, results are nearly identical to LASSO. The ridge model performs the same both linearly and with polynomial features. One notable difference in that percent of persons with college degrees surpasses median household income as the most important feature by permutation importance, but they were very close to begin with. There are some small differences in coefficient weights and what is represented for polynomial features but in general things are very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/4200/0*kuuC8_3Q2YjoLoqt.png)\n",
    "\n",
    "ElasticNet uses the sum of the L1 and L2 norms and acts as a compromise between Ridge and LASSO. Coefficients will sometimes be set to 0, but less commonly than with LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_grid = {\n",
    "    'estimator__alpha': np.logspace(-3, 3, 5),\n",
    "    'estimator__l1_ratio': np.linspace(0, 1, 5),\n",
    "    'poly__degree': [1, 2, 3]\n",
    "             }\n",
    "p1_elastic = linear_model(ElasticNet(), elastic_grid, cv_5, X_train[p1_features], y_train, '1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically the same as the other linear methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Models\n",
    "## Random Forests\n",
    "\n",
    "![](https://miro.medium.com/max/1184/1*i0o8mjFfCn-uD79-F1Cqkw.png)\n",
    "\n",
    "A random forrest is an ensemble classifier/regressor of bad decision trees which average to (hopefully) be a good classfier/regressor. The idea is that although each of these small decision trees perform badly, they all capture *different* information and so average to be a good model of the data. The number of trees in the forest is a hyperparameter, as well as the max depth of the trees. Randomness is injected into these trees in two ways:\n",
    "\n",
    "1. Bagging (Bootstrap Aggregation).  Each tree is trained on a different bootstrapped sample from our training data.\n",
    "2. Restricting number of features each tree can be trained on.  Each tree is only allowed to train on a randomly selected set of features.  The number of features is a hyperparameter.\n",
    "\n",
    "The final classification is made by averaging the individual classifications of each tree in the forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "InteractiveShell.ast_node_interactivity = 'last_expr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Correct Number of Trees\n",
    "\n",
    "More trees in your forest can never *hurt* performance, but eventually you reach a point of diminishing returns where you spend extra time training for very little boost in performance.  We'll evaluate performance while adding trees to see how many we need for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "train_scores = []\n",
    "oob_scores = []\n",
    "test_scores = []\n",
    "\n",
    "rf = RandomForestRegressor(warm_start=True, oob_score=True, random_state=42);\n",
    "estimator_range = range(1, 500, 20)\n",
    "for n_estimators in estimator_range:\n",
    "    rf.n_estimators = n_estimators;\n",
    "    rf.fit(X_train[p1_features], y_train);\n",
    "    train_scores.append(rf.score(X_train[p1_features], y_train));\n",
    "    oob_scores.append(rf.oob_score_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(estimator_range, oob_scores, label=\"Out of Bag scores\")\n",
    "plt.plot(estimator_range, train_scores, label=\"train scores\")\n",
    "plt.ylabel(\"R^2\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.savefig(\"images/warm_start_forest.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'num_trees': estimator_range,\n",
    "              'oob_scores': oob_scores})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're looking at R<sup>2</sup> to the precision of the hundredth, we can pick 150 as a nice even number of estimators.\n",
    "### Parameter Tuning\n",
    "Let's see if we can improve performance by tuning on the number of features and max depth of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=150, oob_score=True, random_state=42)\n",
    "grid = {\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'max_depth': [1, 5, 10, 20, None]\n",
    "}\n",
    "\n",
    "best_score=0\n",
    "for g in ParameterGrid(grid):\n",
    "    rf.set_params(**g)\n",
    "    rf.fit(X_train[p1_features], y_train)\n",
    "    # save if best\n",
    "    if rf.oob_score_ > best_score:\n",
    "        best_score = rf.oob_score_\n",
    "        best_grid = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best R^2 Value: {}\\n\\\n",
    "Best Parameter Grid: {}'.format(best_score, best_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll Use these parameters moving forward with all datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_p1 = RandomForestRegressor(n_estimators=150, max_depth=10, max_features='auto', oob_score=True, random_state=42)\n",
    "rf_p1.fit(X_train[p1_features], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Importance\n",
    "\n",
    "From [A scikit-learn core dev](https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined)\n",
    "\n",
    ">In scikit-learn, we implement the importance as described in [1] (often cited, but unfortunately rarely read...). It is sometimes called \"gini importance\" or \"mean decrease impurity\" and is defined as the total decrease in node impurity (weighted by the probability of reaching that node (which is approximated by the proportion of samples reaching that node)) averaged over all trees of the ensemble.\n",
    "\n",
    "Basically what this comes down to is that if splitting on a feature makes the two leaves below it much more pure (They contain a more concentrated distribution of values), it will be more important than a feature that doesn't increase purity as much, averaged across all trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_p1_imp = pd.DataFrame({\n",
    "    'feature': p1_features,\n",
    "    'importance': rf_p1.feature_importances_ \n",
    "})\n",
    "\n",
    "sns.barplot(x='importance', y='feature', data=rf_p1_imp, orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks different from our results with linear methods in that it gives much more emphasis on the strongest feature, median household income."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(rf_p1, scoring='r2') \\\n",
    "        .fit(X_train[p1_features], y_train, cv='prefit')\n",
    "    \n",
    "plt.figure(figsize=(20,10));\n",
    "plot_perm = perm_plot(perm, p1_features)\n",
    "plt.title('Phase 1 RF Permutation Importance')    \n",
    "plt.show()\n",
    "\n",
    "display(show_weights(perm, feature_names=p1_features))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This permutation importance shows a much more similar result to the results of the linear methods when compared to gini importance. This is a a good reason to be concerned with the accuracy of gini importance; it can exaggerate the importances of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Dependence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import plot_partial_dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_partial_dependence(rf_p1, X_train[p1_features], np.argsort(rf_p1.feature_importances_),\n",
    "    feature_names=p1_features, n_jobs=-1, grid_resolution=50, fig=plt.figure(figsize=(20,10))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are seeing at least some partial dependence based on all features, with the strongest relationships for percent of college graduates and median household income, which is to be expected. One interesting thing to note is that median household income appears almost asymptotic as we get to very high median incomes, which means that the effect only matters up to a certain point.  The effect of college seems more linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting with XGBoost\n",
    "\n",
    "XGBoost is one of the most popular implementation of Gradient Boosted Trees.  This is a method similar to random forests, but rather than using many independent trees it builds trees sequentially, each dependent on the last. It is often done with very shallow trees, or even stumps (trees of depth 1).\n",
    "\n",
    "![](https://www.researchgate.net/profile/Maria_Peraita-Adrados/publication/326379229/figure/fig5/AS:647978477948928@1531501516288/A-simple-example-of-visualizing-gradient-boosting.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_p1 = XGBRegressor(n_jobs=-1)\n",
    "cval_xgb_p1 = cross_validate(xgb_model, X_train[p1_features],\n",
    "    y_train, scoring = 'r2', cv=cv_5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean XGBoost Score: {}\".format(cval_xgb_p1['test_score'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're already getting a score that rivals the tuned Random Forest score with no parameter tuning using XGBoost.  For now we won't tune parameters (as we're more interested in feature importance) but we'll revisit tuning later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_p1.fit(X_train[p1_features].values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_p1_imp = pd.DataFrame({\n",
    "    'feature': p1_features,\n",
    "    'importance': xgb_p1.feature_importances_ \n",
    "})\n",
    "\n",
    "sns.barplot(x='importance', y='feature', data=xgb_p1_imp, orient='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(xgb_p1, scoring='r2') \\\n",
    "        .fit(X_train[p1_features].values, y_train.values, cv='prefit')\n",
    "    \n",
    "plt.figure(figsize=(20,10));\n",
    "plot_perm = perm_plot(perm, p1_features)\n",
    "plt.title('Phase 1 RF Permutation Importance')    \n",
    "plt.show()\n",
    "\n",
    "display(show_weights(perm, feature_names=p1_features)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance by Gini importance is the same for XGBoost and Random Forests, but the XGBoost model puts more emphasis on percent college graduates, and less on percentage of the population that is black.  In fact, this is the first model to put higher influence on percentage hispanic than black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_partial_dependence(xgb_p1, X_train[p1_features].values, np.argsort(xgb_p1.feature_importances_),\n",
    "    feature_names=p1_features, n_jobs=-1, grid_resolution=50, fig=plt.figure(figsize=(20,10))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots look largely the same as the ones for the random forests. It looks like some tracts may have a population density of zero? We'll check that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train['t10_ldb_pop_d'] == 0).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, actually they must just be very small population density values.  In this case it looks like tracts with extremely low population density values have similar life expectancy to those with the highest population density.  However, these values abruptly drop off when they get a little higher, and then slowly climb back up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1 Conclusions\n",
    "\n",
    "**Linear models**:  \n",
    "Perhaps because of the small number of features, we get similar performance from all types of linear models (LASSO, Ridge, ElasticNet). Fully linear models reach an R<sup>2</sup> value of about 0.49, while adding interaction terms and polynomial features up to the order of 3 boost us to an R<sup>2</sup> of around 0.55.  These models all agree that the two most important features are median household income and percent of residents with at least a 4 year degree, with the former slightly outweighing the latter. Both have positive relationships, while the third most important feature (by all measures) is percent non-hispanic black, which has a negative relationship (although it is much less strong than the previous two.\n",
    "\n",
    "**NonLinear Models**\n",
    "Random Forests achieved an R<sup>2</sup> of 0.58 with parameter tuning which XGBoost also achieved with no parameter tuning.  Both models had nearly identical Gini importances of features, which largely agreed with the linear models.  However, when using permutation importance college and income became much closer in importance, and in the gradient boosting model percentage hispanic overtook percentage non-hispanic black for third most important feature.  We can see from the partial dependence plots (as well as plotting this directly against life expectancy, as we did at the beginning), that percent hispanic has a positive relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2\n",
    "\n",
    "Features will all Census and ACS variables in the RECVD priority dataset. \n",
    "\n",
    "**Full variable names**:\n",
    "- t10_ldb_hinc_m - Median household income\n",
    "- t10_ldb_pop_d - Population density\n",
    "- t10_ldb_nhblk_p - Percent non-hispanic black\n",
    "- t10_ldb_hisp_p - Percent hispanic\n",
    "- t10_ldb_col_p - Percent with at least a 4-year degree\n",
    "- t10_ldb_pop_c - Population count\n",
    "- t10_ldb_ag25up_c - Count of persons aged 25 and up\n",
    "- t10_ldb_ag60up_c - Count of persons aged 60 and up\n",
    "- t10_ldb_ag75up_c - Count of persons aged 75 and up\n",
    "- t10_ldb_hh_c - Count of housesholds \n",
    "- t10_ldb_lep_c - Count of persons who speak english not well\n",
    "- t10_ldb_mrenti_m - Median montly rent\n",
    "- t10_ldb_multi_p - Percent of housing units in multi-unit structures\n",
    "- t10_ldb_nhwht_p - Percent non-hispanic white\n",
    "- t10_ldb_asian_p - Percent asian\n",
    "- t10_ldb_fb_p - Percent foreign born\n",
    "- t10_ldb_hs_p - Percent with a high school diploma or less\n",
    "- t10_ldb_unemp_p - Percent unemployed\n",
    "- t10_ldb_npov_c - Count of persons in poverty\n",
    "- t10_ldb_npov_p - Percentage of persons in poverty\n",
    "- t10_ldb_vac_p - Percentage vacant housing units\n",
    "- t10_ldb_own_p - Percentage of owner-occupied housing units\n",
    "- t10_ldb_mhmvali_m - Median home value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_features = [\n",
    "    \"t10_ldb_hinc_m\",\n",
    "\"t10_ldb_pop_d\",\n",
    "\"t10_ldb_nhblk_p\",\n",
    "\"t10_ldb_hisp_p\",\n",
    "\"t10_ldb_col_p\",\n",
    "\"t10_ldb_pop_c\",\n",
    "\"t10_ldb_ag25up_c\",\n",
    "\"t10_ldb_ag60up_c\",\n",
    "\"t10_ldb_ag75up_c\",\n",
    "\"t10_ldb_hh_c\",\n",
    "\"t10_ldb_lep_c\",\n",
    "\"t10_ldb_mrenti_m\",\n",
    "\"t10_ldb_multi_p\",\n",
    "\"t10_ldb_nhwht_p\",\n",
    "\"t10_ldb_asian_p\",\n",
    "\"t10_ldb_fb_p\",\n",
    "\"t10_ldb_hs_p\",\n",
    "\"t10_ldb_unemp_p\",\n",
    "\"t10_ldb_npov_c\",\n",
    "\"t10_ldb_npov_p\",\n",
    "\"t10_ldb_vac_p\",\n",
    "\"t10_ldb_own_p\",\n",
    "\"t10_ldb_mhmvali_m\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
